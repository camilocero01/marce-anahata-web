# Plan de Limpieza - Google Search Console

## üìã Resumen del Problema

Google Search Console reporta **URLs fantasma** que fueron descubiertas en versiones anteriores del sitio pero **ya no existen** en el sitemap actual ni en el c√≥digo.

### Tipos de URLs problem√°ticas:

1. **URLs duplicadas** (versiones en espa√±ol de posts que solo existen en ingl√©s)
2. **P√°ginas de bajo valor SEO** que no deber√≠an indexarse
3. **Archivos duplicados eliminados**

---

## ‚úÖ Correcciones Ya Implementadas

### 1. C√≥digo Fuente (Completado)
- ‚úÖ Corregido `src/pages/blog/[slug].astro` para solo servir posts en espa√±ol
- ‚úÖ Corregido `src/pages/rss.xml.js` para filtrar por idioma
- ‚úÖ Eliminado archivo duplicado `5-tecnica-respiracion-para-dominar-la-anisedad copy.md`
- ‚úÖ Mejorado filtro de sitemap en `astro.config.mjs`

### 2. Robots.txt (Completado)
- ‚úÖ Bloqueadas p√°ginas de bajo valor: `/admin`, `/links`, `/privacidad`
- ‚úÖ Bloqueadas p√°ginas de paginaci√≥n duplicadas: `/blog/page/1`, `/blog/tag/*/page/1`
- ‚úÖ Bloqueadas versiones en ingl√©s de p√°ginas de bajo valor

### 3. Canonical & Hreflang Tags (Completado)
- ‚úÖ Corregido canonical tag para usar path normalizado sin trailing slashes
- ‚úÖ Hreflang alternates ya estaban correctamente configurados
- ‚úÖ Resuelve "Alternate page with proper canonical tag" warning

---

## üõ†Ô∏è Acciones en Google Search Console

### Paso 1: Eliminar URLs Inexistentes (Inmediato)

Google permite solicitar la eliminaci√≥n de URLs que retornan **404** o **410 Gone**.

**Instrucciones:**

1. **Ir a Google Search Console** ‚Üí https://search.google.com/search-console
2. **Seleccionar propiedad:** `https://www.marceanahata.com`
3. **Men√∫ lateral:** Eliminaciones ‚Üí Nueva solicitud
4. **Crear solicitud masiva** para eliminar URLs:

#### URLs a Eliminar (Retornan 404):

**Posts duplicados (versiones en espa√±ol que no existen):**
```
https://www.marceanahata.com/blog/1-conscious-breathing-wellness/
https://www.marceanahata.com/blog/2-healing-power-sound-therapy/
https://www.marceanahata.com/blog/3-why-we-need-rituals/
https://www.marceanahata.com/blog/5-square-breathing-technique/
https://www.marceanahata.com/blog/8-gong-bath-february-2026/
https://www.marceanahata.com/blog/5-tecnica-respiracion-para-dominar-la-anisedad-copy/
```

**P√°ginas de paginaci√≥n duplicadas:**
```
https://www.marceanahata.com/blog/page/1/
https://www.marceanahata.com/en/blog/page/1/
https://www.marceanahata.com/blog/tag/*/page/1/
https://www.marceanahata.com/en/blog/tag/*/page/1/
```

**Nota:** Google elimina las solicitudes en ~24 horas, pero el efecto completo puede tardar 1-2 semanas.

---

### Paso 2: Usar Canonical URLs para Duplicados Leg√≠timos

Para las p√°ginas `/blog/page/1` que son **duplicados leg√≠timos** de `/blog`, asegurar que tienen canonical:

**Verificar en c√≥digo:**
- ‚úÖ `src/pages/blog.astro` debe tener `<link rel="canonical" href="https://www.marceanahata.com/blog/" />`
- ‚úÖ `src/pages/blog/page/[page].astro` para `page=1` debe redirigir a `/blog` o tener canonical

---

### Paso 3: Actualizar Sitemap en GSC (Inmediato)

1. **Ir a:** Sitemaps en Google Search Console
2. **Eliminar sitemap antiguo** (si existe uno problem√°tico)
3. **Agregar sitemap limpio:**
   ```
   https://www.marceanahata.com/sitemap-index.xml
   ```
4. **Reenviar** para forzar re-crawl

---

### Paso 4: Marcar URLs de Bajo Valor como "Noindex" (Opcional)

Si prefieres que Google **nunca indexe** ciertas p√°ginas:

**P√°ginas a marcar con noindex:**
- `/admin`
- `/links`
- `/privacidad`
- `/en/links`
- `/en/privacidad`

**Implementaci√≥n:**

En `src/layouts/Layout.astro`, agregar l√≥gica:

```astro
---
const noindexPages = ['/admin', '/links', '/privacidad', '/en/links', '/en/privacidad'];
const shouldNoindex = noindexPages.some(path => Astro.url.pathname.startsWith(path));
---

<head>
  {shouldNoindex && <meta name="robots" content="noindex, nofollow" />}
  <!-- resto del head -->
</head>
```

---

## üìä Monitoreo y Validaci√≥n

### 1. Verificar que URLs no existen (Retornan 404)

Ejecutar en terminal:

```bash
# Verificar URLs problem√°ticas
$urls = @(
  "https://www.marceanahata.com/blog/1-conscious-breathing-wellness/",
  "https://www.marceanahata.com/blog/2-healing-power-sound-therapy/",
  "https://www.marceanahata.com/blog/5-square-breathing-technique/",
  "https://www.marceanahata.com/blog/5-tecnica-respiracion-para-dominar-la-anisedad-copy/"
)

foreach ($url in $urls) {
  try {
    $response = Invoke-WebRequest -Uri $url -Method Head -ErrorAction Stop
    Write-Host "$url - Status: $($response.StatusCode)" -ForegroundColor Yellow
  } catch {
    Write-Host "$url - Status: 404 ‚úÖ" -ForegroundColor Green
  }
}
```

**Resultado esperado:** Todas retornan **404** (no encontradas).

---

### 2. Verificar Sitemap Limpio

```bash
# Descargar sitemap y verificar
curl https://www.marceanahata.com/sitemap-index.xml

# Verificar que NO contiene URLs problem√°ticas
curl https://www.marceanahata.com/sitemap-0.xml | Select-String "conscious-breathing-wellness"
```

**Resultado esperado:** Solo URLs con prefijo `/en/blog/` para posts en ingl√©s.

---

### 3. Monitorear Google Search Console

**Dashboard ‚Üí Cobertura:**
- **Semana 1:** Las URLs fantasma aparecer√°n como "Eliminadas por solicitud"
- **Semana 2-4:** Las URLs fantasma desaparecer√°n del reporte
- **Mes 1-2:** Google re-crawlear√° el sitio limpio y actualizar√° el √≠ndice

**M√©tricas a revisar:**
- **Valid (V√°lidas):** Debe aumentar (URLs correctas indexadas)
- **Excluded (Excluidas):** Debe aumentar (URLs bloqueadas por robots.txt)
- **Error (Errores):** Debe disminuir a 0
- **Valid with warnings (V√°lidas con advertencias):** Debe disminuir

---

## üîó URLs que S√ç Deber√≠an Indexarse (Whitelist)

Estas URLs **son correctas** y eventualmente deber√≠an indexarse:

**Posts en espa√±ol:**
```
https://www.marceanahata.com/blog/1-bienestar-respiracion/
https://www.marceanahata.com/blog/2-el-poder-sanador-terapia-sonido/
https://www.marceanahata.com/blog/3-por-que-necesitamos-rituales/
https://www.marceanahata.com/blog/4-yoga-y-meditacion-para-tu-trabajo/
https://www.marceanahata.com/blog/8-bano-de-gong-febrero-2026/
```

**Posts en ingl√©s:**
```
https://www.marceanahata.com/en/blog/1-conscious-breathing-wellness/
https://www.marceanahata.com/en/blog/2-healing-power-sound-therapy/
https://www.marceanahata.com/en/blog/3-why-we-need-rituals/
https://www.marceanahata.com/en/blog/4-yoga-meditation-for-work/
https://www.marceanahata.com/en/blog/5-square-breathing-technique/
https://www.marceanahata.com/en/blog/8-gong-bath-february-2026/
```

**P√°ginas principales:**
```
https://www.marceanahata.com/
https://www.marceanahata.com/en/
https://www.marceanahata.com/blog/
https://www.marceanahata.com/en/blog/
https://www.marceanahata.com/clases/
https://www.marceanahata.com/terapia-sonido/
https://www.marceanahata.com/rituales/
https://www.marceanahata.com/bienestar-corporativo/
https://www.marceanahata.com/sobre-mi/
```

---

## ‚è±Ô∏è Timeline Esperado

| Acci√≥n | Plazo | Resultado |
|--------|-------|-----------|
| **Deploy cambios a producci√≥n** | Inmediato | Sitemap limpio, robots.txt actualizado |
| **Reenviar sitemap en GSC** | +1 d√≠a | Google empieza re-crawl |
| **Solicitar eliminaciones en GSC** | +1-2 d√≠as | URLs fantasma marcadas para eliminaci√≥n |
| **Google procesa eliminaciones** | +1 semana | URLs fantasma desaparecen de GSC |
| **Google re-indexa sitio** | +2-4 semanas | URLs correctas indexadas, fantasmas removidas |
| **Estabilizaci√≥n completa** | +1-2 meses | √çndice limpio sin advertencias |

---

## üö® Troubleshooting

### Problema: "URLs a√∫n aparecen despu√©s de 1 semana"

**Soluci√≥n:**
- Verificar que robots.txt est√° desplegado: `curl https://www.marceanahata.com/robots.txt`
- Forzar re-crawl en GSC: Inspeccionar URL ‚Üí Solicitar indexaci√≥n
- Verificar logs de Vercel para errores 404 (deben retornar correctamente)

### Problema: "Google dice que las URLs existen (200 OK)"

**Soluci√≥n:**
- Verificar que el c√≥digo corregido est√° en producci√≥n
- Limpiar cach√© de Vercel/CDN
- Verificar que no hay redirects 301/302 a las URLs fantasma

### Problema: "Tags con espacios no se bloquean en robots.txt"

**Soluci√≥n:**
El wildcard `*` en `Disallow: /blog/tag/*/page/1` no funciona en todos los crawlers.

**Alternativa:** Usar `noindex` meta tag en p√°ginas de paginaci√≥n (ver Paso 4).

---

## üìù Checklist de Deploy

Antes de hacer push a producci√≥n:

- [x] C√≥digo corregido (filtros por idioma)
- [x] Archivo duplicado eliminado
- [x] Robots.txt actualizado
- [x] Sitemap config mejorado
- [ ] **Push a GitHub/Vercel**
- [ ] **Verificar deploy exitoso**
- [ ] **Reenviar sitemap en GSC**
- [ ] **Solicitar eliminaciones en GSC**
- [ ] **Monitorear durante 2 semanas**

---

## üéØ KPIs de √âxito

**Semana 1:**
- ‚úÖ Sitemap limpio desplegado
- ‚úÖ Robots.txt bloqueando p√°ginas de bajo valor
- ‚úÖ Solicitudes de eliminaci√≥n enviadas a GSC

**Mes 1:**
- ‚úÖ 0 URLs fantasma en "Discovered - not indexed"
- ‚úÖ URLs correctas en "Indexed"
- ‚úÖ Coverage report sin errores

**Mes 2-3:**
- ‚úÖ Tr√°fico org√°nico estable o creciente
- ‚úÖ No hay contenido duplicado en rankings
- ‚úÖ Core Web Vitals saludables (LCP < 2.5s, CLS < 0.1)

---

## üìö Referencias

- [Google: Remove URLs](https://support.google.com/webmasters/answer/9689846)
- [Google: Sitemaps](https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview)
- [Robots.txt Specification](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt)
- [Canonical URLs Best Practices](https://developers.google.com/search/docs/crawling-indexing/canonicalization)

---

**√öltima actualizaci√≥n:** 5 de febrero de 2026  
**Estado:** Listo para deploy y limpieza en GSC
